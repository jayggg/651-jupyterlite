{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections and best approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activity is concerned with how a finite element space approximates functions outside it. *Given a function $\\newcommand{\\om}{\\varOmega}$ $u \\in L^2(\\om)$, how best can we approximate it using functions in a finite element subspace $W_{hp}$?*  A quantitative answer to this question can be provided by measuring the **best approximation error** (BAE) \n",
    "$$\n",
    "\\text{BAE}(u) = \\inf_{v \\in W_{hp}} \\| u -v \\|_{L^2(\\om)}.\n",
    "$$\n",
    "We are interested in  seeing how this BAE becomes small as the mesh size $h$ is decreased  and as the polynomial degree $p$ increases. Here \n",
    "$$\n",
    "  W_{hp} =  \\{ w: w|_K \\text{ is a polynomial of degree $\\le p$ on each mesh element }K\\}.\n",
    "$$\n",
    "This space is often called the *discontinuous Galerkin (DG) space* and is your second example of a finite element space. It differs from your first example, the Lagrange finite element space, in that functions in $W_{hp}$ can be discontinous. Clearly, $W_{hp}$ is a subspace of $L^2(\\om)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "To compute BAE, we use orthogonal projections. Recall two properties of orthogonal projections we proved in the lectures.  Let $X$ be an inner product space over $\\mathbb R$ and let $P: X \\to X$ be an orthogonal projection onto a subspace $\\newcommand{\\ran}{\\text{range}}$  $M = \\ran(P)$ of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Property 1.*    $Px$ is the **best approximation** to an $x \\in X$ from the subspace $M$,   namely, \n",
    "$$\n",
    "\\| x - Px \\|_X = \\inf_{m \\in M} \\| x - m \\|_X.\n",
    "$$\n",
    "The right hand side is the BAE$(x)$ of $x$ from $M$. Thus Property 1\n",
    "implies that we can compute BAE$(x)$ if we can compute $Px$ and the norm of the difference  $\\| x - Px \\|_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Property 2.*  $Px$ satisfies \n",
    "$$\n",
    "(Px , m)_X = (x, m)_X\n",
    "$$\n",
    "for all $x \\in X$ and $m \\in M$. This equation helps us compute $P x$, as we shall now illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the $L^2$-orthogonal projection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q_{hp}$ denote the $\\newcommand{\\om}{\\varOmega}$ $L^2(\\om)$-orthogonal projection onto $W_{hp}$.\n",
    "Given a $u \\in L^2(\\om)$ we want to compute the function $q = Q_{hp} u$ in $W_{hp}$. By Property 2, \n",
    "$$\n",
    "(q, m)_{L^2(\\om)} = (u, m)_{L^2(\\om)} \n",
    "$$\n",
    "for all $m \\in W_{hp}$.\n",
    "\n",
    "We will encounter many more equations of this type in this course. They are all of the form\n",
    "$$\n",
    "a(q, m) = b(m).\n",
    "$$\n",
    "For the projection, $a(\\cdot,\\cdot)$ is the just the inner product $(\\cdot, \\cdot)_{L^2(\\om)}$, while $b(\\cdot)$ equals $(u, \\cdot)_{L^2(\\om)}$.\n",
    "It is useful to get acquainted with some terminology related to equations like $a(q, m) = b(m)$  rightaway:\n",
    "- The set of functions from which the solution $q$  is to be found is called the set of **trial functions**.\n",
    "- The set of functions $m$ for which the equation must hold is called the set of  **test functions**. (In the projection example, the spaces of trial and test functions are the same, but they need not be so in other examples.)\n",
    "- The right hand side is called a **linear form** since it is linear in the test function $m$. (In the projection example, it contains the problem data involving the known function $u$.)\n",
    "- The left handside is called a **bilinear form** (since it is linear in both the trial function $q$ and the test function $m$).\n",
    "\n",
    "*You will see these names being used for certain ngsolve objects below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngsolve as ng\n",
    "from netgen.occ import unit_square\n",
    "from ngsolve import dx, x, y, sin, BilinearForm, LinearForm, Mesh\n",
    "\n",
    "def ProjectL2(u, W):\n",
    "    \"\"\" Input: u as a CF.\n",
    "        Output: L^2-projection Q u as a GF. \"\"\"\n",
    "    Qu = ng.GridFunction(W)\n",
    "    q = W.TrialFunction()\n",
    "    m = W.TestFunction()\n",
    "    a = BilinearForm(q*m*dx)\n",
    "    b = LinearForm(u*m*dx)\n",
    "    \n",
    "    with ng.TaskManager():\n",
    "        a.Assemble()\n",
    "        b.Assemble()\n",
    "        Qu.vec.data = a.mat.Inverse(inverse='sparsecholesky') * b.vec\n",
    "    return Qu    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining the code, let us visualize a smooth function and its projection into the space of piecewise constant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve.webgui import Draw\n",
    "\n",
    "h = 0.2\n",
    "mesh = ng.Mesh(unit_square.GenerateMesh(maxh=h))\n",
    "p = 0    # piecewise constant DG space\n",
    "Whp = ng.L2(mesh, order=p)\n",
    "\n",
    "u = sin(5 * x * y)\n",
    "Qu = ProjectL2(u, Whp)\n",
    "\n",
    "Draw(u, mesh); Draw(Qu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to a matrix problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, notice the `Assemble` and `Inverse` methods. The first converts equations of the form \n",
    "$$\n",
    "a(q, m) = b(m)\n",
    "$$\n",
    "for $q, m$ in some space $W$ into a matrix system, once a basis is set, as explained next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\phi_1, \\ldots, \\phi_N$ form a set of basis functions of $W$. Then expanding the unknown function $q \\in W$ in that basis, \n",
    "$$\n",
    "q = \\sum_{j=1}^N X_j \\phi_j,\n",
    "$$\n",
    "we find that, with $m=\\phi_i$, the equations \n",
    "$$\n",
    "\\sum_{j=1}^N a(\\phi_j, \\phi_i)\\, X_j = b(\\phi_i)\n",
    "$$\n",
    "hold for every $i=1, \\ldots, N$. Hence, defining a matrix $A \\in \\mathbb{R}^{N \\times N}$ and a vector $B \\in \\mathbb{R}^N$ by \n",
    "$$\n",
    "A_{ij} = a(\\phi_j, \\phi_i), \\qquad B_i = b(\\phi_i),\n",
    "$$\n",
    "we have converted the problem of finding $q \\in W$ into a problem of finding a vector $X \\in \\mathbb{R}^N$ satisfying \n",
    "$$\n",
    "A X = B.\n",
    "$$\n",
    "This is the process that happens behind the scenes when the `Assemble` method is called. In the code for the functoin `ProjectL2`,  the object `a.mat` contains the matrix $A$ (in a sparse format) and `b.vec` contains the vector $B$ after the `Assemble` methods are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $X$, we need to invert the matrix $A$, namely $X = A^{-1} B$. This is what happens when the method  `a.mat.Inverse` is called. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive mesh refinements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to study the best approximation error over a collection of meshes whose *element shapes* do not vary too much over the collection, even if their *element sizes* might vary considerably. A sequence of increasingly finer meshes where the element angles never change from those of the coarsest mesh is obtained by a simple *uniform refinement*. This is illustrated in the sequence of mesh images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.3))\n",
    "Draw(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.ngmesh.Refine(); Draw(mesh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.ngmesh.Refine(); Draw(mesh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this process of uniform refinement, when started on any coarse mesh, does not generate any new angles within the new elements, i.e., each triangular element of this infinite sequence of meshes is similar (in the geometric sense) to one of the triangles in the initial coarse mesh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections on uniform refinements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the $L^2$ projection on a sequence of uniform refinements, starting from a very coarse mesh, using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import Integrate, sqrt\n",
    "import numpy as np\n",
    "\n",
    "def ProjectOnSuccessiveRefinements(u, p=0, hcoarse=1, nrefinements=8):                                   \n",
    "    \"\"\"Project to f.e. spaces on a sequence of uniformly refined meshes.\"\"\"\n",
    "    \n",
    "    Qus = []; errors = []; meshes = []; ndofs = []\n",
    "    mesh = ng.Mesh(unit_square.GenerateMesh(maxh=hcoarse))\n",
    "    \n",
    "    for ref in range(nrefinements): \n",
    "        W = ng.L2(mesh, order=p)\n",
    "        Qu = ProjectL2(u, W)         \n",
    "        sqrerr = (Qu - u)**2\n",
    "      \n",
    "        Qus.append(Qu) \n",
    "        errors.append(sqrt(Integrate(sqrerr, mesh)))\n",
    "        meshes.append(ng.Mesh(mesh.ngmesh.Copy()))\n",
    "        ndofs.append(W.ndof) \n",
    "      \n",
    "        mesh.ngmesh.Refine()\n",
    "\n",
    "    return Qus, np.array(errors), ndofs, meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select an infinitely smooth function $u$ for the initial experiments (and you will experiment with less regular functions in exercises). We select a smooth $u$ with some oscillations  (as otherwise the approximation errors rapidly become too close to machine precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = sin(5*x*y)\n",
    "Qus, es, ns, _ = ProjectOnSuccessiveRefinements(u, p=0)\n",
    "errors = {0: es}; ndofs = {0: ns}\n",
    "es  # display the sequence of error norms just computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this sequence of error norms are decreasing, approximately halving at each step. This immediately gives us an indication of the expected rate of convergence of the error.  But anticipating more general error sequences in other case, let us get organized on rate computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate rate of convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate at what rate the errors on successive refinements (stored in the above list `es`) go to zero.  Let $e_i$ denote the $i$th element of the list.\n",
    "\n",
    "\n",
    "Specifically, what we would like to determine is the number $r$, the **rate of convergence**, such that the errors are bounded by $O(h^r).$\n",
    "If the sequence $e_i$ goes to zero like $O(h_i^r)$, then  since the refinement pattern dictates\n",
    "$$\n",
    "h_i = \\frac{h_0}{2^i}, \n",
    "$$ \n",
    "we should see $e_{i+1} / e_i  \\sim O(2^{-r})$. Hence to estimate $r$, we compute \n",
    "$$\n",
    "\\log_2 \\frac{e_{i+1}}{ e_i}.\n",
    "$$\n",
    "These logarithms are computed and tabulated together with the error numbers using the formatting function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small module; attempt install if not found\n",
    "try:\n",
    "    from prettytable import PrettyTable\n",
    "except ModuleNotFoundError:\n",
    "    try:\n",
    "        # this works only on pyodide\n",
    "        import micropip\n",
    "        await micropip.install(\"prettytable\")\n",
    "        from prettytable import PrettyTable\n",
    "    except ModuleNotFoundError:\n",
    "        # works locally, on jupyterhub, etc\n",
    "        !pip3 install prettytable\n",
    "        from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TabulateRate(name, dat, h0=1):\n",
    "    \"\"\"Inputs: \n",
    "        name = Name for second (error norm) column, \n",
    "        dat = list of error data on successive refinements\n",
    "        log2h0inv = log2(1/h0), where h0 is coarsest meshsize.\n",
    "    \"\"\"\n",
    "    col = ['h', name, 'rate']\n",
    "    t = PrettyTable()\n",
    "    h0col = ['%g'%h0]\n",
    "    t.add_column(col[0], h0col + [h0col[0] + '/' + str(2**i) \n",
    "                                  for i in range(1, len(dat))])\n",
    "    t.add_column(col[1], ['%.12f'%e for e in dat])\n",
    "    t.add_column(col[2], ['*'] + \\\n",
    "                 ['%1.2f' % r \n",
    "                  for r in np.log(dat[:-1]/dat[1:])/np.log(2)])\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply this function to tabulate rates of convergence for each $p$ using a series of uniformly refined meshes. Both the error norms and the rate computed using the log-technique are tabulated for each degree considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p=0$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabulateRate('L2norm(Pu-u)', es)  # compute rate & tabulate previous p=0 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p=1$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayL2BAE(u, p=1):\n",
    "    _, es, ns, _ = ProjectOnSuccessiveRefinements(u, p=p)\n",
    "    errors[p] = es; ndofs[p] = ns  # store for later use\n",
    "    TabulateRate('L2norm(Pu-u)', es)\n",
    "\n",
    "DisplayL2BAE(u, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p=2$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p=3$ case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p=4$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayL2BAE(u, p=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These observations clearly suggest that the rate of convergence appears to be $p+1$ when polynomial degree $p$ is used. \n",
    "\n",
    "Indeed, we will prove in a later lecture that\n",
    "$$\n",
    "\\inf_{ w \\in W_{hp}} \\| u - w \\|_{L^2(\\om)} \\le O(h^{p+1}).\n",
    "$$\n",
    "Rigorous approximation estimates like these form a basic pillar of finite element theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs. degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the above computations, we have also stored $\\dim(W_{hp})$, or the **number of degrees of freedom**. This is useful when trying to gauge how much bang for the buck we obtain when using various meshes and various orders $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "for p in range(5):\n",
    "    plt.loglog(ndofs[p], errors[p], '.-', label='p=%d'%p)\n",
    "\n",
    "plt.xlabel('Degrees of Freedom')\n",
    "plt.ylabel('$L^2$ best approximation error'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from these results that for the smooth `u` under consideration,  the accuracy of its best approximation, measured *per* degree of freedom increases (quite dramatically) when using higher $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b><font color=red>Exercise:</font></b>  Give a formula for the dimension of $W_{hp}$ in terms of a mesh quantity and $p$.\n",
    "</div>\n",
    "\n",
    "\n",
    "*Note:*  We stored this dimension, for the meshes and the $p$-values we used in the above computations, in the   `ndofs` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b><font color=red>Exercise:</font></b>  Modify the above computations for the Lagrange finite element space  $V_{hp} = \\{ v: v $ is continuous and $\n",
    "v|_K $ is a polynomial of degree $\\le p$ on each mesh element $K\\}$ for $p=1, 2, \\ldots, 4$.  At what rate does the best approximation error \n",
    "$$\n",
    "\\inf_{ w \\in V_{hp}} \\| u - w \\|_{L^2(\\om)} \n",
    "$$\n",
    "appear to converge to zero? \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b><font color=red>Exercise:</font></b>\n",
    "Modify the above computations to determine the rate of convergence of the best approximation error of the Lagrange finite element space $V_{hp}$ in the following  stronger norm\n",
    "$$\n",
    "\\| u \\|_{H^1(\\om)} := \\left( \\| u \\|_{L^2(\\om)}^2 + \\| \\partial_x u\\|_{L^2(\\om)}^2 + \\| \\partial_y u\\|_{L^2(\\om)}^2 \\right)^{1/2}.\n",
    "$$\n",
    "At what rate does the best approximation error \n",
    "$$\n",
    "\\inf_{ w \\in V_{hp}} \\| u - w \\|_{H^1(\\om)} \n",
    "$$\n",
    "appear to converge to zero? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr>\n",
    "    \n",
    "    \n",
    "$\\ll$ [Table Of Contents](./0_INDEX.ipynb) <br>\n",
    "$\\ll$ [Jay Gopalakrishnan](http://web.pdx.edu/~gjay/)\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": ""
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
